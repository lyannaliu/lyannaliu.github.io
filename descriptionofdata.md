---
layout: page
permalink: /descriptionofdata/index.html
title: Description of Data
---

# Description of Data

## Contents

* [1. Summary](#1)
* [2. Million Playlist](#2)
    * [2.1 Data Source](#2.1)
    * [2.2 Description of the Raw Data](#2.2)
    * [2.3 Exploratory Data Analysis](#2.3)
* [3. LastFM](#3)
    * [3.1 Data Source](#3.1)
    * [3.2 Description of the Raw Data](#3.2)
    * [3.3 Exploratory Data Analysis](#3.3)


<h2 id="1">1. Summary</h2>
We used Last.FM Dataset and Million Playlist Dataset as our two data sets. The Last.fm Dataset contains song tags (eg genres and/or official playlists, Bay Area top 100 or Hip Hop) and song/track similarity for all of the tracks in the Million Song Dataset and SQLite databases that can be parsed. The Million Playlist Dataset contains one million playlists generated by Spotify users from January 2010 to October 2017 in JSON format. 


<h2 id="2">2. Million Playlist</h2>

<h3 id="2.1">2.1 Data Source</h3>
We obtained the Million Playlist Dataset from: http://recsys-challenge.spotify.com/

<h3 id="2.2">2.2 Description of the Raw Data</h3>
The Million Playlist data set contains:

  - One million playlists generated by Spotify users from January 2010 to October 2017 (in JSON format)
  - Attributes include: 
    - Playlist name
    - Number of tracks included
    - Number of followers 
    - Each song includes artist name, track name, duration (ms), album name, along with their respective URIs
  - Numerical Variables are calculated dynamically depending on the target song and include the following (further detailed in the EDA): 
    - Related song frequency
    - Related artist frequency
    - Related album frequency
    - Total song frequency
    - Total artist frequency
    - Total album frequency
    
A snippet of the Million Playlist data is shown here:

![Total Counts](/images/MP_data_example.png)

<h3 id="2.3">2.3 Exploratory Data Analysis</h3>

The first step to analyzing the Million Playlist Data was to decide the data structure that would best suit the needs of both the Last.FM Model and Million Playlist Model. The biggest issue with the playlist data was ensuring that tracks could be identified as unique while keeping the amount of loaded information to a minimum (to reduce memory costs).

There were several instances of tracks with the same song name, and several instances of tracks with the same song and artist but different albums. Therefore, we made the following decisions:

- Remastered/Remixes of the same song performed by the same artists are considered different tracks.
- The same song performed by different artists are considered different tracks.
- The same songs performed by the same artist in different albums are considered different tracks.

The most obvious way to store a track was as a `namedtuple` with three properties - song, artist, and album.

```python
Track = namedtuple("Track", ["song", "artist", "album"])
```

Using this template, we loaded each playlist as a list of Track `namedtuple`s, and the entire list of playlists as a list of lists.

```python
def randomly_load_files():
    all_playlists = []
    all_files = glob.glob("mpd.v1/data/100000/*.json")
    for file in all_files:
        with open(file) as f:
            data = json.load(f)
            all_playlists.extend([[Track(song=track['track_name'], artist=track['artist_name'], 
                album=track['album_name']) for track in playlist['tracks']] for playlist in data['playlists']])
    return all_playlists
```

Another challenge we faced was memory restrictions. Loading the full list of 1,000,000 playlists wasn't feasible, so we trimmed the data down to just 100,000 playlists. Out of these 100,000 playlists, 90% were designated train and 10% were designated test. After loading the data, we achieved this random split using `sklearn`'s `train_test_split` method.

```python
all_playlists = randomly_load_files()
detailed_train_playlists, detailed_test_playlists = train_test_split(all_playlists, train_size=.9)
```

The training and test sets were pickled and used for both the Last.FM and Million Playlist models. This handled the issue of creating train and test sets of data.

```python
with open('detailed_train_playlists.pkl', 'wb') as f:
    pickle.dump(detailed_train_playlists, f)

with open('detailed_test_playlists.pkl', 'wb') as f:
    pickle.dump(detailed_test_playlists, f)
```

The next question we needed to answer was how to turn completely categorical data with too many variables to 1-hot encode into preferably numerical data that could be ingested by a model.

We began by exploring the concept of calculating total counts of songs, artists, and albums from the training set as potential attributes for a track.

```python
# get counts of all unique songs, artists, and albums
def get_unique(playlist_list):
    totalArtists = Counter()
    totalAlbums = Counter()
    details = Counter()
    for playlist in playlist_list:
        for track in playlist:
            totalArtists[track.artist] += 1
            totalAlbums[track.album] += 1
            details[track] += 1
    return (totalArtists, totalAlbums, details)

totalArtistCount, totalAlbumCount, songDetails = get_unique(detailed_train_playlists)
```

The above function returns three `Counter`s (that function similarly to a dictionary) that contain the number of times each unique track, artist, and album appears across all training playlists.

```python
# Count of tracks
songDetails.most_common()
[(Track(song='HUMBLE.', artist='Kendrick Lamar', album='DAMN.'), 3984),
 (Track(song='One Dance', artist='Drake', album='Views'), 3844),
 (Track(song='Closer', artist='The Chainsmokers', album='Closer'), 3730),
 (Track(song='Broccoli (feat. Lil Yachty)', artist='DRAM', album='Big Baby DRAM'),
  3679),
 (Track(song='Congratulations', artist='Post Malone', album='Stoney'), 3538),
 (Track(song='Caroline', artist='Aminé', album='Good For You'), 3172),
 (Track(song='iSpy (feat. Lil Yachty)', artist='KYLE', album='iSpy (feat. Lil Yachty)'),
  3135),
 (Track(song='Location', artist='Khalid', album='American Teen'), 3103),
 (Track(song='XO TOUR Llif3', artist='Lil Uzi Vert', album='Luv Is Rage 2'),
  3100),
 (Track(song='Bad and Boujee (feat. Lil Uzi Vert)', artist='Migos', album='Culture'),
  3051),
 (Track(song='No Role Modelz', artist='J. Cole', album='2014 Forest Hills Drive'),
  2899),
 (Track(song='Bounce Back', artist='Big Sean', album='I Decided.'), 2860),
 (Track(song='Ignition - Remix', artist='R. Kelly', album='Chocolate Factory'),
  2860),
  ...

# Count of artists
totalArtistCount.most_common()

[('Drake', 74492),
 ('Kanye West', 37094),
 ('Kendrick Lamar', 31078),
 ('Rihanna', 30070),
 ('The Weeknd', 28218),
 ('Eminem', 25941),
 ('Ed Sheeran', 24748),
 ('Future', 22572),
 ('J. Cole', 21718),
 ('Justin Bieber', 21235),
 ('Beyoncé', 21235),
 ('The Chainsmokers', 19982),
 ('Chris Brown', 18625),
 ('Luke Bryan', 18580),
 ('Twenty One Pilots', 17924),
 ('Calvin Harris', 17851),
 ('Lil Uzi Vert', 17612),
 ('Post Malone', 17240),
 ...

 # Count of albums
 [('Views', 18436),
 ('Stoney', 13837),
 ('Greatest Hits', 13504),
 ('More Life', 12396),
 ('DAMN.', 12341),
 ('Beauty Behind The Madness', 12287),
 ('Coloring Book', 11868),
 ('American Teen', 10906),
 ('Culture', 10712),
 ('The Life Of Pablo', 10214),
 ('Purpose', 10114),
 ('2014 Forest Hills Drive', 9766),
 ('Starboy', 9566),
 ('Blurryface', 9488),
 ('ANTI', 9350),
 ('÷', 9190),
 ('Original Album Classics', 9185),
 ('x', 8885),
 ('Montevallo', 8815)
 ...
 ```

 Next, we were interested in visualizing any trends for these counts by plotting the calculated frequencies.

```python
# Plot total track, artist, and album counts
count_types = [songDetails, totalArtistCount, totalAlbumCount]
count_type_names = ['Tracks', 'Artists', 'Albums']

fig, ax = plt.subplots(nrows=3, ncols=1)
fig.set_size_inches(10, 15)
fig.suptitle('Total Counts of Tracks, Artists, and Albums', fontsize=20, y=0.95)

for count_type, count_type_name, i in zip(count_types, count_type_names, range(3)):
    ax[i].scatter(range(len(count_type)), count_type.values(), label=count_type_name)
    ax[i].set_xlabel('Indexes')
    ax[i].set_ylabel('Total count')
    ax[i].set_title('Total counts of {}'.format(count_type_name))
```

![Total Counts](/images/total_counts.png)

By doing so we discovered that all three follow a similar Pareto distribution. Colinearity became a concern, so we selected ten tracks and compared the trend of the total song count, artist count, and album counts of each song.

```python
# Plot subset of counts
from matplotlib.lines import Line2D

colors = ['blueviolet', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue', 'darkorchid', 'fuchsia',
         'indianred', 'mediumblue', 'lightpink']
markers = [Line2D([], [], marker='.', markersize=10, label='Song Count'),
           Line2D([], [], marker='x', markersize=10, label='Artist Count'), 
           Line2D([], [], marker='o', markersize=10, label='Album Count')]

fig, ax = plt.subplots()
fig.set_size_inches(10, 5)
fig.suptitle('Total Counts of a Subset of Tracks, Artists, and Albums', fontsize=20, y=0.95)

for i in range(10):
    ax.scatter(i, songDetails[detailed_train_playlists[0][i]], marker='.', color=colors[i])
    ax.scatter(i, totalArtistCount[detailed_train_playlists[0][i].artist], marker='x', color=colors[i])
    ax.scatter(i, totalAlbumCount[detailed_train_playlists[0][i].album], marker='o', color=colors[i])

ax.set_xlabel('Indexes')
ax.set_ylabel('Total count')
ax.legend(handles=markers)
```

![Count Comparison](/images/count_comparison.png)

The plot shows that songs with high counts do not necessarily have artists and albums with high counts. The same holds true for the relationship between artists and albums to songs. Therefore, even though there is some visible colinearity, we determined that the frequency counts were not perfectly colinear and could potentially be significant as separate predictors.

<h2 id="3">3. LastFM</h2>
<h3 id="3.1">3.1 Data Source</h3>
We obtained the Last.fm Dataset on the Million Song Dataset: https://labrosa.ee.columbia.edu/millionsong/lastfm. 

<h3 id="3.2">3.2 Description of the Raw Data</h3>
The Last.fm data set contains:

  - Song tags (eg genres and/or official playlists, Bay Area top 100 or Hip Hop) and song/track similarity for all of the tracks in the Million Song Dataset
  - SQLite databases that can be parsed
  - Numerical Variables: 
    - Artist Hotness 
    - Artist Familiarity
    - Duration
    - Similarity
  - Categorical Variables:
    - Track ID
    - Artist Name
    - Same Album
    - Tags
    - Year of release

##### Processing
To alleviate storage/memory limitations, this dataset was built by querying tables from the SQLite databases made available by the Million Song Dataset team. The two databases used were: [Tags](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/lastfm/lastfm_tags.db) and [Track Metadata](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db). 

Tables were pulled from each database, using variations of the code below (particular to a table and database):

```python
conn = sqlite3.connect("sql_db/lastfm_tags.db")
tid_tag_table = pd.read_sql_query("select * from tid_tag;", conn)
track_ids = pd.read_sql_query("select * from tids;", conn)
tags = pd.read_sql_query("select * from tags;", conn)
conn.close()

conn = sqlite3.connect("sql_db/track_metadata.db")
songs_table = pd.read_sql_query("select * from songs;", conn)
conn.close()
```

Appropriate modifications were made to ensure the tables from each database could be merged, with the below code:

```python
tid_num = np.linspace(1, len(track_ids), len(track_ids))
tid_num = [int(x) for x in tid_num]
track_ids['tid_num'] = tid_num
track_ids_copy = track_ids.copy()
track_ids_copy.rename(columns={'tid':'track_id'}, inplace=True)
tid_tag_table_copy = tid_tag_table.copy()
trackid_tagnums = pd.merge(tid_tag_table_copy, track_ids_copy, left_on = 'tid', right_on = 'tid_num')

tag_num = np.linspace(1, len(tags), len(tags))
tag_num = [int(x) for x in tag_num]
tags['tag_num'] = tag_num
tags.rename(columns={'tag':'tag_name'}, inplace=True)

tracks_tags = pd.merge(trackid_tagnums, tags, left_on = 'tag', right_on = 'tag_num')
tracks_tags_copy = tracks_tags.copy()
```
Subsequently, the tables were merged to create one large dataset to begin exploring and cleaning: 
```python
songs_tags = pd.merge(tracks_tags_copy, songs_table, left_on = 'track_id', right_on = 'track_id')
```
This dataset has one row per track per tag. Ultimately, this dataset will be processed to create one row per track, containing all relevant features.

##### Cleaning
The messiest feature of the LastFM dataset is the tag names. Many tags are misspelled, modified with adjectives that don’t change the genre/tag meaning, and/or are completely irrelevant. This was initially discovered by grouping the track and tag data by tag and counting the number of tracks associated with each. The discrepancies were seen in tags with very low frequency (ie n = 1). 

The following steps were taken to clean the tags:
-	We looked through tags that had a frequency of 1; any tags that could be recoded to a different category (ie ‘-pop’ to ‘pop’ or ‘jazzy’ to ‘jazz’) were recoded. This process was done in a manual iterative process with code similar to the  below:

```python
songs_tags.loc[songs_tags['tag_name'].str.contains('symph*onic.+metal', case = False), 
                       'tag_name'] = 'symphonic metal'
songs_tags.loc[songs_tags['tag_name'].str.contains('romantic', case = False), 
                       'tag_name'] = 'romantic'
```

-	Once all of the tags were recoded, we looked through the remaining tags with a frequency of 1 and determined they couldn’t be recoded to a broader genre. These tags (and the associated track, as the initial merged dataset had one line per track per tag) were removed from the dataset. The below code was used to review the tracks:

```python
tag_frequency = songs_tags.groupby(by = 'tag_name', as_index = False).agg({
    'track_id': np.count_nonzero
})
idx = np.where(tag_frequency['track_id'] <= 1)
tag_frequency['tag_name'].iloc[idx]
```
![Tag Frequency](/images/tag_frequency.png)

-	The distribution of tags changed upon this cleaning; initially, rock was the most frequent tag, but pop and jazz are now more frequent than rock.

```python
tag_frequency = songs_tags.groupby(by = 'tag_name', as_index = False).agg({
    'track_id': np.count_nonzero
})
sorted_tags = tag_frequency.sort_values('track_id', ascending = False)
sorted_tags.rename(columns={'track_id':'count'}, inplace=True)
top_50 = tag_frequency.sort_values('track_id', ascending = False)[:50]

fig, ax = plt.subplots(1, 1, figsize = (90,90))

ypos = top_50['tag_name']
track_count = top_50['track_id']

ax.barh(ypos, track_count)
ax.invert_yaxis()  # labels read top-to-bottom
ax.tick_params(labelsize = 70)
ax.set_xlabel('Count of Associated Tracks', fontsize = 80)
ax.set_ylabel('Tag Names', fontsize = 80)

ax.set_xlabel('Number of Associated Tracks')
ax.set_title('Top 50 Tag Names', fontsize = 90)

plt.savefig('figures/top50tags.png')
plt.show()
```
![Distribution of Tags](/image/top50tags.png)

-	More cleaning could likely still be done on the tags by searching through other low frequency tags. This is an endeavor left for future work.

Other cleaning procedures were conducted to account for data that should have been represented as NaN and not zero:
-	When initially visualizing the distribution of ‘artist hotness’, there were some values = 0. Since artist hotness would be > 0 (even if it’s still small), those values were replaced with NaN.
-	When initially visualizing the distribution of ‘year’, there were values = 0. Since year cannot = 0 for recorded songs, those values were replaced with NaN.
```python
songs_tags['artist_hotness'] = songs_tags['artist_hotness'].replace(0, np.NaN)
songs_tags['year'] = songs_tags['year'].replace(0, np.NaN)
```
##### Generation of final 'database' 

After all of the initial processing and cleaning, a few more steps were taken to create the final dataset with one row per track.

First, all of the tags for a particular track were concatenated together into one list with the following code, to create one row per track:
```python
single_songs = songs_tags.groupby(['track_id'])['tag_name'].apply(lambda x: ','.join(x)).reset_index()
single_songs
```
![Single songs](/image/single_songs.png)

Next, columns/features that won't be used in analysis were removed from the base dataset:
```python
songs_tags_copy = songs_tags.copy()
songs_otherfeatures = songs_tags_copy.drop(['tid', 'tag', 'val', 'tid_num', 'tag_name', 'tag_num',
                                      'song_id', 'artist_id', 'artist_mbid'], axis = 1)
songs_otherfeatures.head()
```
![Other features](/image/songs_otherfeatures.png)

The final dataset (which was later sorted by song and title) was created by joining the 'single_song' dataframe with the 'songs_otherfeatures' frame and dropping any duplicates (since songs_otherfeatures still has multiple rows with the same track):
```python
single_songs_copy = single_songs.copy()
songs_otherfeatures = songs_otherfeatures.drop_duplicates(keep = 'first')
singlesong_features = single_songs.join(songs_otherfeatures.set_index('track_id'), 
                                        on = 'track_id', how = 'left')
singlesong_features.head()
```
![Singlesong_features](/image/singlesong_features.png)

<h3 id="3.3">3.3 Exploratory Data Analysis</h3>

Various smaller grouped tables were created to explore the data, including: number of tags associated with each track, distribution of artist hotness, distribution of song duration, etc.

Some of the relevant code and figures are shown here:
```python
songs_tags.rename(columns={'artist_hotttnesss':'artist_hotness'}, inplace=True)

songs_tags['artist_hotness'] = songs_tags['artist_hotness'].replace(0, np.NaN)

hotness_artists = songs_tags.groupby(by = 'artist_name', as_index = False).agg({
    'artist_hotness': np.max
})

fig, ax = plt.subplots(1, 1, figsize = (10, 7))

ax.hist(hotness_artists['artist_hotness'], bins = 30, range = [np.min(hotness_artists['artist_hotness']), 
                                                              np.max(hotness_artists['artist_hotness'])]);

ax.tick_params(labelsize = 16)
ax.set_xlabel('Artist Hotness', fontsize = 18)
ax.set_ylabel('Number of Artists', fontsize = 18)
ax.set_title('Distribution of Artist Hotness', fontsize = 20)

plt.savefig('figures/artist_hotness.png')
plt.show()
```
![Artist hotness](/image/artist_hotness.png)

As you can see, artist hotness has a fairly normal distribution.

```python
fig, ax = plt.subplots(1, 1, figsize = (10, 7))

ax.hist(singlesong_features['artist_familiarity'], range = 
        [np.min(singlesong_features['artist_familiarity']), 
         np.max(singlesong_features['artist_familiarity'])], bins = 30)

ax.tick_params(labelsize = 16)
ax.set_xlabel('Artist Familiarity', fontsize = 18)
ax.set_ylabel('Number of Songs', fontsize = 18)
ax.set_title('Distribution of Artist Familiarity across Songs', fontsize = 20)

plt.savefig('figures/artist_familiarity.png')
plt.show()
```
![Artist familiarity](/image/artist_familiarity.png)
Artist familiarity also has a normal distribution.

As is explained in the Models and Methods section, the two primary methods of identification 'similar songs' for the LastFM model is by tag or artist. In the plot farther up this page, you can see the spread of the top 50 tags. Below, it is apparent that the tags have an exponential dis



```python
unique_artists = singlesong_features.groupby(['artist_name'], as_index = False).agg({
    'track_id': np.count_nonzero
})
artists_sorted = unique_artists.sort_values('track_id', ascending = False)[0:50]

fig, ax = plt.subplots(1, 1, figsize = (90,100))

ypos = artists_sorted['artist_name']
track_count = artists_sorted['track_id']

ax.barh(ypos, track_count)
ax.invert_yaxis()  # labels read top-to-bottom
ax.tick_params(labelsize = 70)
ax.set_xlabel('Count of Associated Tracks', fontsize = 80)
ax.set_ylabel('Artists Names', fontsize = 80)

ax.set_xlabel('Number of Associated Tracks')
ax.set_title('Top 50 Artists', fontsize = 90)

plt.savefig('top50_artists.png')
plt.show()
```

**LIZ TO COMPLETE THIS**




