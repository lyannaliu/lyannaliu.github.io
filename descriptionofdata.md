---
layout: page
permalink: /descriptionofdata/index.html
title: Description of Data
---

# Description of Data

* [1. Summary](#1)
* [2. LastFM](#2)
    * [2.1 Data Source](#2.1)
    * [2.2 Description of the Raw Data](#2.2)
    * [2.3 Exploratory Data Analysis](#2.3)
* [3. Million Playlist](#3)
    * [3.1 Data Source](#3.1)
    * [3.2 Description of the Raw Data](#3.2)
    * [3.3 Exploratory Data Analysis](#3.3)

<h2 id="1">1. Summary</h2>
We used Last.FM Dataset and Million Playlist Dataset as our two data sets. The Last.fm Dataset contains song tags (eg genres and/or official playlists, Bay Area top 100 or Hip Hop) and song/track similarity for all of the tracks in the Million Song Dataset and SQLite databases that can be parsed. The Million Playlist Dataset contains one million playlists generated by Spotify users from January 2010 to October 2017 in JSON format. 

<h2 id="2">2. LastFM</h2>
<h3 id="2.1">2.1 Data Source</h3>
We obtained the Last.fm Dataset on the Million Song Dataset: https://labrosa.ee.columbia.edu/millionsong/lastfm. 

<h3 id="2.2">2.2 Description of the Raw Data</h3>
The Last.fm data set contains:

  - Song tags (eg genres and/or official playlists, Bay Area top 100 or Hip Hop) and song/track similarity for all of the tracks in the Million Song Dataset
  - SQLite databases that can be parsed
  - Numerical Variables: 
    - Artist Hotness 
    - Artist Familiarity
    - Duration
    - Similarity
  - Categorical Variables:
    - Track ID
    - Artist Name
    - Same Album
    - Tags
    - Year of release

<h3 id="2.3">2.3 Exploratory Data Analysis</h3>

##### Processing
To alleviate storage/memory limitations, this dataset was built by querying tables from the SQLite databases made available by the Million Song Dataset team. The two databases used were: [Tags](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/lastfm/lastfm_tags.db) and [Track Metadata](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db). 

Tables were pulled from each database and joined to create a dataset that had one row per track_id per tag, so there were multiple rows per track, depending on how many tags were associated with each track. 



Various smaller grouped tables were created to explore the data, including: number of tags associated with each track, distribution of artist hotness, distribution of song duration, etc.

The final dataset (ie one row per song) was then created by concatenating all tags for one song into a row (basically a row of tag strings now associated with each song, instead of multiple rows associated with each song, each containing a tag). This dataset will serve as the database that will be queried when running the model, using playlists from the Million Playlist Dataset (MPD).

##### Cleaning
The messiest feature of the LastFM dataset is the tag names. Many tags are misspelled, modified with adjectives that don’t change the genre/tag meaning, and/or are completely irrelevant. This was initially discovered by grouping the track and tag data by tag and counting the number of tracks associated with each. The discrepancies were seen in tags with very low frequency (ie n = 1). 

The following steps were taken to clean the tags:
-	We looked through tags that had a frequency of 1; any tags that could be recoded to a different category (ie ‘-pop’ to ‘pop’ or ‘jazzy’ to ‘jazz’) were recoded.
-	Once all of those tags were recoded, we looked through the remaining tags with a frequency of 1 and determined they couldn’t be recoded to a broader genre. These tags (and the associated track, as the initial merged dataset had one line per track per tag) were removed from the dataset. 
-	The distribution of tags changed upon this cleaning; initially, rock was the most frequent tag, but pop and jazz are now more frequent than rock.
-	More cleaning could likely still be done on the tags by searching through other low frequency tags. This is an endeavor left for future work.

Other cleaning procedures were conducted to account for data that should have been represented as NaN and not zero:
-	When initially visualizing the distribution of ‘artist hotness’, there were some values = 0. Since artist hotness would be > 0 (even if it’s still small), those values were replaced with NaN.
-	When initially visualizing the distribution of ‘year’, there were values = 0. Since year cannot = 0 for recorded songs, those values were replaced with NaN.



<h2 id="3">3. Million Playlist</h2>

<h3 id="3.1">3.1 Data Source</h3>
We obtained the Million Playlist Dataset from: http://recsys-challenge.spotify.com/

<h3 id="3.2">3.2 Description of the Raw Data</h3>
The Million Playlist data set contains:

  - One million playlists generated by Spotify users from January 2010 to October 2017 (in JSON format)
  - Attributes include: 
    - Playlist name
    - Number of tracks included
    - Number of followers 
    - Each song includes artist name, track name, duration (ms), album name, along with their respective URIs
  - Numerical Variables are calculated dynamically depending on the target song and include the following (further detailed in the EDA): 
    - Related song frequency
    - Related artist frequency
    - Related album frequency
    - Total song frequency
    - Total artist frequency
    - Total album frequency
    
**Needs to be completed:** 

Include one dictionary/df to illustrate - Haley
Issues with the data?
Steps taken to clean
Code used to clean data

<h3 id="3.3">3.3 Exploratory Data Analysis</h3>

The first step to analyzing the Million Playlist Data was to decide the data structure that would best suit the needs of both the Last.FM Model and Million Playlist Model. The biggest issue with the playlist data was ensuring that tracks could be identified as unique while keeping the amount of loaded information to a minimum (to reduce memory costs).

There were several instances of tracks with the same song name, and several instances of tracks with the same song and artist but different albums. Therefore, we made the following decisions:

- Remastered/Remixes of the same song performed by the same artists are considered different tracks.
- The same song performed by different artists are considered different tracks.
- The same songs performed by the same artist in different albums are considered different tracks.

The most obvious way to store a track was as a `namedtuple` with three properties - song, artist, and album.

```python
Track = namedtuple("Track", ["song", "artist", "album"])
```

Using this template, we loaded each playlist as a list of Track `namedtuple`s, and the entire list of playlists as a list of lists.

```python
def randomly_load_files():
    all_playlists = []
    all_files = glob.glob("mpd.v1/data/100000/*.json")
    for file in all_files:
        with open(file) as f:
            data = json.load(f)
            all_playlists.extend([[Track(song=track['track_name'], artist=track['artist_name'], 
                album=track['album_name']) for track in playlist['tracks']] for playlist in data['playlists']])
    return all_playlists
```

Another challenge we faced was memory restrictions. Loading the full list of 1,000,000 playlists wasn't feasible, so we trimmed the data down to just 100,000 playlists. Out of these 100,000 playlists, 90% were designated train and 10% were designated test. After loading the data, we achieved this random split using `sklearn`'s `train_test_split` method.

```python
all_playlists = randomly_load_files()
detailed_train_playlists, detailed_test_playlists = train_test_split(all_playlists, train_size=.9)
```

The training and test sets were pickled and used for both the Last.FM and Million Playlist models. This handled the issue of creating train and test sets of data.

```python
with open('detailed_train_playlists.pkl', 'wb') as f:
    pickle.dump(detailed_train_playlists, f)

with open('detailed_test_playlists.pkl', 'wb') as f:
    pickle.dump(detailed_test_playlists, f)
```

The next question we needed to answer was how to turn completely categorical data with too many variables to 1-hot encode into preferably numerical data that could be ingested by a model.

We began by exploring the concept of calculating total counts of songs, artists, and albums from the training set as potential attributes for a track.

```python
# get counts of all unique songs, artists, and albums
def get_unique(playlist_list):
    totalArtists = Counter()
    totalAlbums = Counter()
    details = Counter()
    for playlist in playlist_list:
        for track in playlist:
            totalArtists[track.artist] += 1
            totalAlbums[track.album] += 1
            details[track] += 1
    return (totalArtists, totalAlbums, details)

totalArtistCount, totalAlbumCount, songDetails = get_unique(detailed_train_playlists)
```

The above function returns three `Counter`s (that function similarly to a dictionary) that contain the number of times each unique track, artist, and album appears across all training playlists.

```python
# Count of tracks
songDetails.most_common()
[(Track(song='HUMBLE.', artist='Kendrick Lamar', album='DAMN.'), 3984),
 (Track(song='One Dance', artist='Drake', album='Views'), 3844),
 (Track(song='Closer', artist='The Chainsmokers', album='Closer'), 3730),
 (Track(song='Broccoli (feat. Lil Yachty)', artist='DRAM', album='Big Baby DRAM'),
  3679),
 (Track(song='Congratulations', artist='Post Malone', album='Stoney'), 3538),
 (Track(song='Caroline', artist='Aminé', album='Good For You'), 3172),
 (Track(song='iSpy (feat. Lil Yachty)', artist='KYLE', album='iSpy (feat. Lil Yachty)'),
  3135),
 (Track(song='Location', artist='Khalid', album='American Teen'), 3103),
 (Track(song='XO TOUR Llif3', artist='Lil Uzi Vert', album='Luv Is Rage 2'),
  3100),
 (Track(song='Bad and Boujee (feat. Lil Uzi Vert)', artist='Migos', album='Culture'),
  3051),
 (Track(song='No Role Modelz', artist='J. Cole', album='2014 Forest Hills Drive'),
  2899),
 (Track(song='Bounce Back', artist='Big Sean', album='I Decided.'), 2860),
 (Track(song='Ignition - Remix', artist='R. Kelly', album='Chocolate Factory'),
  2860),
  ...

# Count of artists
totalArtistCount.most_common()

[('Drake', 74492),
 ('Kanye West', 37094),
 ('Kendrick Lamar', 31078),
 ('Rihanna', 30070),
 ('The Weeknd', 28218),
 ('Eminem', 25941),
 ('Ed Sheeran', 24748),
 ('Future', 22572),
 ('J. Cole', 21718),
 ('Justin Bieber', 21235),
 ('Beyoncé', 21235),
 ('The Chainsmokers', 19982),
 ('Chris Brown', 18625),
 ('Luke Bryan', 18580),
 ('Twenty One Pilots', 17924),
 ('Calvin Harris', 17851),
 ('Lil Uzi Vert', 17612),
 ('Post Malone', 17240),
 ...

 # Count of albums
 [('Views', 18436),
 ('Stoney', 13837),
 ('Greatest Hits', 13504),
 ('More Life', 12396),
 ('DAMN.', 12341),
 ('Beauty Behind The Madness', 12287),
 ('Coloring Book', 11868),
 ('American Teen', 10906),
 ('Culture', 10712),
 ('The Life Of Pablo', 10214),
 ('Purpose', 10114),
 ('2014 Forest Hills Drive', 9766),
 ('Starboy', 9566),
 ('Blurryface', 9488),
 ('ANTI', 9350),
 ('÷', 9190),
 ('Original Album Classics', 9185),
 ('x', 8885),
 ('Montevallo', 8815)
 ...
 ```

 Next, we were interested in visualizing any trends for these counts by plotting the calculated frequencies.

```python
# Plot total track, artist, and album counts
count_types = [songDetails, totalArtistCount, totalAlbumCount]
count_type_names = ['Tracks', 'Artists', 'Albums']

fig, ax = plt.subplots(nrows=3, ncols=1)
fig.set_size_inches(10, 15)
fig.suptitle('Total Counts of Tracks, Artists, and Albums', fontsize=20, y=0.95)

for count_type, count_type_name, i in zip(count_types, count_type_names, range(3)):
    ax[i].scatter(range(len(count_type)), count_type.values(), label=count_type_name)
    ax[i].set_xlabel('Indexes')
    ax[i].set_ylabel('Total count')
    ax[i].set_title('Total counts of {}'.format(count_type_name))
```

![Total Counts](/images/total_counts.png)

By doing so we discovered that all three follow a similar Pareto distribution. Colinearity became a concern, so we selected ten tracks and compared the trend of the total song count, artist count, and album counts of each song.

```python
# Plot subset of counts
from matplotlib.lines import Line2D

colors = ['blueviolet', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue', 'darkorchid', 'fuchsia',
         'indianred', 'mediumblue', 'lightpink']
markers = [Line2D([], [], marker='.', markersize=10, label='Song Count'),
           Line2D([], [], marker='x', markersize=10, label='Artist Count'), 
           Line2D([], [], marker='o', markersize=10, label='Album Count')]

fig, ax = plt.subplots()
fig.set_size_inches(10, 5)
fig.suptitle('Total Counts of a Subset of Tracks, Artists, and Albums', fontsize=20, y=0.95)

for i in range(10):
    ax.scatter(i, songDetails[detailed_train_playlists[0][i]], marker='.', color=colors[i])
    ax.scatter(i, totalArtistCount[detailed_train_playlists[0][i].artist], marker='x', color=colors[i])
    ax.scatter(i, totalAlbumCount[detailed_train_playlists[0][i].album], marker='o', color=colors[i])

ax.set_xlabel('Indexes')
ax.set_ylabel('Total count')
ax.legend(handles=markers)
```

![Count Comparison](/images/count_comparison.png)

The plot shows that songs with high counts do not necessarily have artists and albums with high counts. The same holds true for the relationship between artists and albums to songs. Therefore, even though there is some visible colinearity, we determined that the frequency counts were not perfectly colinear and could potentially be significant as separate predictors.
